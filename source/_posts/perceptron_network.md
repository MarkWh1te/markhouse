---
title: 无痛入门神经网络(1) 感知器 
date: 2017-06-08 01:17:00
tags:
  - deep learning
  - machine learning
  - data mining
categories: algorithms
---
 
## 综述 

> And God said, “Let there be light,” and there was light.

这个系列的主要目标是讲清楚什么是神经网络以及它的原理。既然题目是无痛入门，我会用尽量简单的数学讲明白问题。在遇到一定要用到的复杂数学知识时，我会提前做好介绍。编程方面，我也会最简单的Python代码实现。即这个系列对读者的最低要求为有求常见函数导数和可以用Python写出for循环的能力。
## 感知器的历史
现在即使是很多没有接触过神经网络的人也知道神经网络由于架构的不同分为 深度神经网络(DNN),积卷神经网络(CNN)，循环神经网络(RNN)和对抗神经网络(GAN)等，但是在上世纪六十年代的时候，并没有神经网络这个名词,那时候只有一种被称作感知器的学习工具，在最开始的时候它很看起来很有前景。但是在1969年，Marvin Minsky在名为<<perceptron>>的书中分析了它的功能和它的局限性，导致了当时很多人对感知器以及神经网络嗤之以鼻。然而感知器的学习过程依然在广泛使用，比如google就在很多产品中使用了相关算法。
## 感知器的架构
一个感知器的架构如下图所示，
![perceptron](http://7xq2dq.com1.z0.glb.clouddn.com/Screen%20Shot%202017-06-07%20at%209.45.09%20PM%20%281%29.png)
分别为:
* 输入向量（input),即为用来训练感知器的原始数据
* 阶梯函数(stop function),可以通过生物上的神经元阈值来理解，当输入向量和权重相乘之后，如果结果大于阈值（比如0），则神经元激活(返回1)，反之则神经元未激活(返回0)
* 权重(weight),感知器通过数据训练，学习到的权向量通过将它和输入向量点乘，把乘积带入阶梯函数后我们可以得到我们期待的结果


## 感知器的训练过程
感知器训练过程可以分为两步：
1. 在输入向量中加入一列都为1的偏置列（bias）这是因为有可能我们选取的特值肯定不是完全反应了客观规律的，偏置列可以理解为对特值的补充，也就是在设计这个系统的时候没有考虑到的偏差 。比如我们都要设计一个来表示数学中或运算的感知器，它的真值表如下

|x1|x2|结果|
|----|------|----|
| 0| 0|  0|
| 0| 1|  1|
| 1| 0|  1|
| 1| 1|  1|
那么相对应的Python代码生成的输入向量和期待结果应该为

```python
from numpy import array
training_data = [
    (array([0,0,1]), 0),
    (array([0,1,1]), 1),
    (array([1,0,1]), 1),
    (array([1,1,1]), 1),
]
```
2. 随机选择输入向量中的一个执行下面的步骤，重复n次。
    * 计算权重向量和一组输入向量的点乘，用乘积计算阶梯含函数的结果
    * 如果结果和目标结果一样，则不更改权重向量
    * 如果结果和目标结果不同且为0，则把权重向量和输入向量相加，把结果作为新的权重向量
    * 如果结果和目标结果相同且为1，则把权重向量和输入向量相减，把结果作为新的权重向量
    
在步骤重复若干次后， 我们可以得到一个不变权重向量，即收敛结束。有了这个权重向量我们就可以用新的输入向量来预测结果了。
具体代码为：

```python
from random import choice
from numpy import array,dot,random

#随机生成权重向量
weight = random.rand(3)
#重复100次
n = 100
#阶梯函数 
unit_step = lambda x:0 if x < 0 else 1
#训练过程 
for i in range(n):
    x,expect = choice(training_data)
    output = dot(x,weight)
    flag = expect - unit_step(output)
    weight += flag * x
#结果展示
for x, _ in training_data:
  results = dot(x,weight)
  print("{}:{} -> {}".format(x[:2],results,unit_step(results)))
```
如果取第一步中的traning_data训练，就可以得到一个符合或真值表的感知器了
## 感知器的局限性
在一开始讲感知器历史的时候我就说过感知器是有局限性的，举个例子，如果我们要让感知器来判断x1,x2，是否一样，我们可以想到这样的一组训练数据

相同: (1,1)->1,(0,0)->1
 
不相同: (1,0)->0,(0,1)->0

用Python向量来表示就是
```python
from numpy import array
training_data = [
    (array([0,0,1]), 1),
    (array([0,1,1]), 0),
    (array([1,0,1]), 0),
    (array([1,1,1]), 1),
]
```
但是如果你用这个training_data带入刚刚的代码训练，你会发现每次执行之后的感知器判断的结果都不一样，换而言之，这次没有收敛，我们没有得到一个稳定的权重向量。

我们从代数上证明感知器没有办法完成这件事情，首先假设权重向量为 {w1,w2},那根据训练数据和向量乘法法则我们可以得到下面四个不等式:

$$
w_1 \cdot 0 + w_2 \cdot 0 = 0 >= 0 
$$
$$
w_1 \cdot 0 + w_2 \cdot 1 < 0 
$$
$$
w_1 \cdot 1 + w_2 \cdot 0 < 0 
$$
$$
w_1 \cdot 1 + w_2 \cdot 1 >= 0 
$$


显而易见这是不可能成立的。

## 局限性的原因和解决办法

> 无论个体还是集体，过于单一化只会走向毁灭 ----攻殻機動隊 (1995)

* 原因
    1. 从感知器的架构可以看出，感知器只有两层，一层输入一层输出，训练只会改变第一层的权重向量
    2. 感知器的第一层只做了一次线性变换
    
    这两个原因导致感知器训练结果的好坏大部分取决与输入向量的质量，即是否选了足够重要的特征值作为输入向量。这是一个在做深度学习过程中很容易犯的一个错误，因为这个思路等于是把解决问题的最难的一步让人类而不是算法处理了。所以一但发现大部分的时间是在设计和寻找更重要的特征值时， 应该考虑换个思路
* 解决办法
如果我们想将输入向量质量对训练结果的影响降低，那么最容易想到的办法是增加层数，有更多的权重向量可以记录更多的信息。但这还不够，因为是单纯的增加层数没有解决单一化的问题。我们需要的是一个多层的，每一层都是非线性的神经网络。这样除了输入层和输出层，中间的隐藏层可以帮助我去分辨输入向量中的哪个纬度重要，这样就不会有感知器那样的局限性。但是新的问题就是我们如何去训练一个这样复杂的神经网络呢？嗯，下次我会讲讲这个的。







